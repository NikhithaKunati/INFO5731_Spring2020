{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikhithaKunati/INFO5731_Spring2020/blob/master/Copy_of_INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "# Write your code here\n",
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n",
        "####input your credentials here\n",
        "consumer_key = 'AIbJbo8gLAvOaYG7kAahokZgC'\n",
        "consumer_secret = '8CfDXlvK2jemTawCvkBJIBqGMKHtFQzU47gsFaCyoQMOagEjLB'\n",
        "access_token = '1863154260-dQYfsfNerNMyxqJSb1lyD5OCac36k4qsaV6ETXS'\n",
        "access_token_secret = 'YPNSa2PbNxLiZ1EoswmIP75oi6QqeCG79Ud0ESixdsRTd'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "# Open/Create a file to append data\n",
        "csvFile = open('wuhancoronavirus.csv', 'a')\n",
        "data = pd.read_csv('wuhancoronavirus.csv', names=['tweet'], header=None)\n",
        "#Use csv Writer\n",
        "csvWriter = csv.writer(csvFile)\n",
        "for tweet in tweepy.Cursor(api.search,q=\"#wuhancoronavirus\",lang=\"en\",since=\"2020-02-01\").items(100):\n",
        "  #print (tweet.created_at, tweet.text)\n",
        "  csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/wuhancoronavirus.csv',error_bad_lines=False,names=['tweet'])\n",
        "\n",
        "#Lower casing\n",
        "data['tweet']=data['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "#Removing Punctuation\n",
        "data['tweet'] = data['tweet'].str.replace('[^A-Za-z ]', '')\n",
        "\n",
        "#remove numbers\n",
        "data['tweet'] = data['tweet'].str.replace('\\d+', '')\n",
        "\n",
        "#remove unwanted \"brt\" from text\n",
        "data['tweet'] = data['tweet'].str.replace('brt', '')\n",
        "\n",
        "\n",
        "#Removing Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop=stopwords.words('english')\n",
        "data['tweet']=data['tweet'].apply(lambda x:\" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st=PorterStemmer()\n",
        "data['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "#Lemmetization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "data['tweet']=data['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "#saving to .txt file\n",
        "data['tweet'].to_csv('clean.txt',header=False,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INczo4sRQdn",
        "colab_type": "text"
      },
      "source": [
        "Convert the **.txt** file to **.csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xuYtvc0KNw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the .txt file to .csv\n",
        "df = pd.read_csv('/content/clean.txt',sep =' ',delimiter='\\n')\n",
        "df.to_csv('clean.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47mnfMDNwMMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "nltk.download('punkt')\n",
        "raw = str(data['tweet'])\n",
        "\n",
        "#tokenize\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "\n",
        "#Parts of Speech tagging\n",
        "token = nltk.pos_tag(tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTAS1qarF57X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c8573eb-c003-4ccc-bb31-57068914dcc2"
      },
      "source": [
        "#total verb, past participle taken\n",
        "entity_VBN = [item for item in token if item[1] =='VBN'] \n",
        "vbn = len(entity_VBN)\n",
        "vbn"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsvvwcyTEp01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "970d9958-d2df-465c-c2ba-484c047b5572"
      },
      "source": [
        "#total dates in text\n",
        "entity_dates = [item for item in token if item[1] =='JJ' or item[1] =='CD'] \n",
        "dates = len(entity_dates)\n",
        "dates"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H7Um6EOL-x-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a092ff9-3231-45c8-d18e-fd89e62a4f96"
      },
      "source": [
        "#total nouns \n",
        "entity_noun = [item for item in token if item[1] =='NN'] \n",
        "noun = len(entity_noun)\n",
        "noun"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvdXTHqEMR0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8a69353-56a6-435e-e9ff-b1942563d757"
      },
      "source": [
        "#total number of adjectives\n",
        "entity_JJ = [item for item in token if item[1] =='JJ'] \n",
        "jj = len(entity_JJ)\n",
        "jj"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_NPSHGiMUmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d31931a7-ac08-451d-c8c9-d2d2b8e35646"
      },
      "source": [
        "#total NNS, noun plurals\n",
        "entity_NNS = [item for item in token if item[1] =='NNS'] \n",
        "nns = len(entity_NNS)\n",
        "nns"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehmomrr9MW_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00b60407-2f30-47ac-9f66-cdc41ca8754c"
      },
      "source": [
        "#total verbs, present\n",
        "entity_VBP = [item for item in token if item[1] =='VBP'] \n",
        "vbp = len(entity_VBP)\n",
        "vbp"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW-n9bdyOw2P",
        "colab_type": "text"
      },
      "source": [
        "Constituency parsing aims to extract a constituency-based parse tree from the given sentence. For example, given a sentence: \"I do not like sleeping for long hours\"; in the given sentence, I is a 'NP' and the remaining sentence is a 'VP'. On further dividing we get: I is 'PRP', do is 'VBP', not is 'RB' etc.,\n",
        "\n",
        "Where as for dependency parsing tree, phrasal constituents and phrase-structure rules do not play a direct role. Instead, the syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words\n",
        "\n",
        "\n"
      ]
    }
  ]
}